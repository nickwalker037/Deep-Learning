{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to visualize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(28, 28), cmap='gray')\n",
    "    plt.title('y = ' + str(data_sample[1].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and print the train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Print the training dataset:\n",
      "  Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "print(\"Print the training dataset:\\n \", train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and print the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the validating dataset:\n",
      "  Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "print(\"Print the validating dataset:\\n \", validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each picture is represented as the values in the pixel matrix shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter3/3.32_image_values.png\" width=\"550\" alt=\"MNIST elements\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image:  None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The image: \", show_data(train_dataset[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll flatten the tensor as shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter3/3.3.2Imagetovector2.png\" width=\"550\" alt=\"Flattern Image\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Creating a Softmax Classifier </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the input and output size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(SoftMax, self).__init__()\n",
    "        self.linear=nn.Linear(in_size, out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftMax(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(input_dim, output_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:  torch.Size([10, 784])\n",
      "b:  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print('W: ', list(model.parameters())[0].size())\n",
    "print('b: ', list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So the actual image is 10x784 -- one parameter vector for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the learning rate, optimizer, loss criterion (cross-entropy), and data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train the model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "N_test = len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x.view(-1, 28*28)) #convert square images to tensor\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        correct = 0\n",
    "        \n",
    "        # perform a prediction on the validation data:\n",
    "        for x_test, y_test in validation_loader:\n",
    "            z = model(x_test.view(-1, 28*28))\n",
    "            _, yhat = torch.max(z.data, 1) #get classification column with highest value\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        accuracy = correct / N_test\n",
    "        loss_list.append(loss.data)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('iteration number: ', epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Analyze Results </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and accuracy on validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(loss_list, color = color)\n",
    "ax1.set_xlabel('epoch', color=color)\n",
    "ax1.set_ylabel('total loss', color = color)\n",
    "ax1.tick_params(axis = 'y', color = color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)\n",
    "ax2.plot(accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', color=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first 5 misclassified samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for x, y in validation_dataset:\n",
    "    z = model(x.reshape(-1, 28 * 28))\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    if yhat != y:\n",
    "        show_data((x, y))\n",
    "        plt.show()\n",
    "        print(\"yhat: \",yhat)\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
